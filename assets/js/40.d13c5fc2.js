(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{356:function(t,e,a){"use strict";a.r(e);var r=a(11),n=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("blockquote",[e("p",[t._v("欢迎关注 "),e("a",{attrs:{href:"https://www.zhihu.com/people/b2aa86a2a1d6f226ac4e03b5da990887",target:"_blank",rel:"noopener noreferrer"}},[t._v("@机器学习社区"),e("OutboundLink")],1),t._v(" ，专注学术论文、大模型落地实践、人工智能、机器学习算法")])]),t._v(" "),e("p",[t._v("作者：Fareed Khan")]),t._v(" "),e("p",[t._v("Transformer架构可能看起来很恐怖，您也可能在YouTube或博客中看到了各种解释。但是下面，将通过提供一个全面的数学示例阐明它的原理。通过这样做，我希望简化对Transformer架构的理解。")]),t._v(" "),e("p",[t._v("那就开始吧！")]),t._v(" "),e("p",[e("strong",[t._v("---\x3e成立了大模型技术讨论群，第一时间获取最新学术、技术资讯、技术讨论交流，移至文末加入我们")])]),t._v(" "),e("h2",{attrs:{id:"inputs-and-positional-encoding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#inputs-and-positional-encoding"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Inputs and Positional Encoding")])]),t._v(" "),e("p",[t._v("让我们解决最初的部分，在那里我们将确定我们的输入并计算它们的位置编码。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic2.zhimg.com/v2-ecb267fbf56850458d81bfb7c1843a85_b.jpg",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"step-1-defining-the-data"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-1-defining-the-data"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Step 1 (Defining the data)")])]),t._v(" "),e("p",[t._v("第一步是定义我们的数据集 (语料库)。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-7f42712be82fe8ab536421cbc0fad7f8_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("在我们的数据集中，有3个句子 (对话) 取自《权力的游戏》电视剧。尽管这个数据集看起来很小，但它已经足以帮助我们理解之后的数学公式。")]),t._v(" "),e("h2",{attrs:{id:"step-2-finding-the-vocab-size"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-2-finding-the-vocab-size"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Step 2 (Finding the Vocab Size)")])]),t._v(" "),e("p",[t._v("为了确定词汇量，我们需要确定数据集中的唯一单词总数。这对于编码 (即将数据转换为数字) 至关重要。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-39890982c621e5ce174771aa51a281d2_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("其中N是所有单词的列表，并且每个单词都是单个token，我们将把我们的数据集分解为一个token列表，表示为N。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-75029b2a3e74e9fc4373106a6723659f_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("获得token列表 (表示为N) 后，我们可以应用公式来计算词汇量。")]),t._v(" "),e("p",[t._v("具体公式原理如下：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-d9af3c4c6bcf0b28474d8d49c213e21e_b.jpg",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-da6981850d675c3bb3f1af4b34255b4a_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("使用set操作有助于删除重复项，然后我们可以计算唯一的单词以确定词汇量。因此，词汇量为23，因为给定列表中有23个独特的单词。")]),t._v(" "),e("h2",{attrs:{id:"step-3-encoding-and-embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-3-encoding-and-embedding"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Step 3 (Encoding and Embedding)")])]),t._v(" "),e("p",[t._v("接下来为数据集的每个唯一单词分配一个整数作为编号。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-c5567a2d14f4c9d849a8569f45a9ba38_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("在对我们的整个数据集进行编码之后，是时候选择我们的输入了。我们将从语料库中选择一个句子以开始:")]),t._v(" "),e("p",[t._v("“When you play game of thrones”")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-b2616f5c80d07722692912ef43f4b988_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("作为输入传递的每个字将被表示为一个编码，并且每个对应的整数值将有一个关联的embedding联系到它。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-1b38af73013f04cc54319b98bfb5dc92_b.jpg",alt:""}})]),t._v(" "),e("ul",[e("li",[t._v("这些embedding可以使用谷歌Word2vec (单词的矢量表示) 找到。在我们的数值示例中，我们将假设每个单词的embedding向量填充有 (0和1) 之间的随机值。")]),t._v(" "),e("li",[t._v("此外，原始论文使用embedding向量的512维度，我们将考虑一个非常小的维度，即5作为数值示例。")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-ee5fe44a04938c1cb8c8f9574c166f6b_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("现在，每个单词embedding都由5维的embedding向量表示，并使用Excel函数RAND() 用随机数填充值。")]),t._v(" "),e("h2",{attrs:{id:"step-4-positional-embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-4-positional-embedding"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Step 4 (Positional Embedding)")])]),t._v(" "),e("p",[t._v("让我们考虑第一个单词，即 “when”，并为其计算位置embedding向量。位置embedding有两个公式:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic2.zhimg.com/v2-e99f2ff4e1c8daa53a10a58b72d6d37d_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("第一个单词 “when” 的POS值将为零，因为它对应于序列的起始索引。此外，i的值 (取决于是偶数还是奇数) 决定了用于计算PE值的公式。维度值表示embedding向量的维度，在我们的情形下，它是5。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-3d5178caa22d201e2ea39acd1ec27163_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("继续计算位置embedding，我们将为下一个单词 “you” 分配pos值1，并继续为序列中的每个后续单词递增pos值。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-418381190d40f0195c9e56fb6c6701f0_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("找到位置embedding后，我们可以将其与原始单词embedding联系起来。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic2.zhimg.com/v2-4d7e4f5541e25be1ef7e4cebeb095b9d_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("我们得到的结果向量是e1+p1，e2+p2，e3+p3等诸如此类的embedding和。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-d09790faf3508d0ffd412f6e7c7d3e32_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("Transformer架构的初始部分的输出将在之后用作编码器的输入。")]),t._v(" "),e("h2",{attrs:{id:"编码器"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#编码器"}},[t._v("#")]),t._v(" "),e("strong",[t._v("编码器")])]),t._v(" "),e("p",[t._v("在编码器中，我们执行复杂的操作，涉及查询（query），键（key）和值（value）的矩阵。这些操作对于转换输入数据和提取有意义的表示形式至关重要。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-545609ef42cf591f4437d418c620101b_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("在多头注意力（multi-head attention）机制内部，单个注意层由几个关键组件组成。这些组件包括:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-9e7d68580ceaef973f47cdd44785a820_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("请注意，黄色框代表单头注意力机制。让它成为多头注意力机制的是多个黄色盒子的叠加。出于示例的考虑，我们将仅考虑一个单头注意力机制，如上图所示。")]),t._v(" "),e("h2",{attrs:{id:"step-1-performing-single-head-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-1-performing-single-head-attention"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Step 1 (Performing Single Head Attention)")])]),t._v(" "),e("p",[t._v("注意力层有三个输入")]),t._v(" "),e("ul",[e("li",[t._v("Query")]),t._v(" "),e("li",[t._v("Key")]),t._v(" "),e("li",[t._v("Value")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic2.zhimg.com/v2-1157d56afcfd37893b83c8be52b6d8f5_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("在上面提供的图中，三个输入矩阵 (粉红色矩阵) 表示从将位置embedding添加到单词embedding矩阵的上一步获得的转置输出。另一方面，线性权重矩阵 (黄色，蓝色和红色) 表示注意力机制中使用的权重。这些矩阵的列可以具有任意数量的维数，但是行数必须与用于乘法的输入矩阵中的列数相同。在我们的例子中，我们将假设线性矩阵 (黄色，蓝色和红色) 包含随机权重。这些权重通常是随机初始化的，然后在训练过程中通过反向传播和梯度下降等技术进行调整。所以让我们计算 (Query, Key and Value metrices):")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-4daa2337b9fceb369fa76ab634fb4193_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("一旦我们在注意力机制中有了query, key, 和value矩阵，我们就继续进行额外的矩阵乘法。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-8c753d5c4b197bbf344c6c162ce63906_b.jpg",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic2.zhimg.com/v2-3c5d8287ceab24ace304733047181115_b.jpg",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic1.zhimg.com/v2-55118ecd90c56549eaa2e112643228c4_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("现在，我们将结果矩阵与我们之前计算的值矩阵相乘:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-66dfdfa3f2e389da409c9ea1bfe1cf62_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("如果我们有多个头部注意力，每个注意力都会产生一个维度为 (6x3) 的矩阵，那么下一步就是将这些矩阵级联在一起。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic3.zhimg.com/v2-f3e6618465f45203e5b9f10d0adff18e_b.jpg",alt:""}})]),t._v(" "),e("p",[t._v("在下一步中，我们将再次执行类似于用于获取query, key, 和value矩阵的过程的线性转换。此线性变换应用于从多个头部注意获得的级联矩阵。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://pic4.zhimg.com/v2-64efbaf71b18afe72a9288c90e20b8d7_b.jpg",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"技术交流群"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#技术交流群"}},[t._v("#")]),t._v(" "),e("strong",[t._v("技术交流群")])]),t._v(" "),e("p",[t._v("建了前沿、实战技术交流群！想要进交流群、获取资料的同学，可以直接加微信号："),e("strong",[t._v("mlc2060")]),t._v("。加的时候备注一下："),e("strong",[t._v("研究方向 +学校/公司+知乎")]),t._v("，即可。然后就可以拉你进群了。")]),t._v(" "),e("p",[e("strong",[t._v("强烈推荐大家关注")]),t._v(" "),e("a",{attrs:{href:"https://www.zhihu.com/people/chen-xi-63-33-5",target:"_blank",rel:"noopener noreferrer"}},[t._v("机器学习社区"),e("OutboundLink")],1),t._v(" "),e("strong",[t._v("知乎账号和")]),t._v(" "),e("a",{attrs:{href:"https://mp.weixin.qq.com/s/moRNkCaBDtYZgXwaZDXBwA",target:"_blank",rel:"noopener noreferrer"}},[t._v("机器学习社区"),e("OutboundLink")],1),t._v(" "),e("strong",[t._v("微信公众号，可以快速了解到最新优质文章。")])]),t._v(" "),e("p",[t._v("前沿技术资讯、算法交流、求职内推、算法竞赛、面试交流(校招、社招、实习)等、与 10000+来自港科大、北大、清华、中科院、CMU、腾讯、百度等名校名企开发者互动交流~")]),t._v(" "),e("h2",{attrs:{id:"推荐文章"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#推荐文章"}},[t._v("#")]),t._v(" 推荐文章")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/650261112",target:"_blank",rel:"noopener noreferrer"}},[t._v("Meta AI 提出指令回译新方法，击败现有全部LLaMa模型！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/647083894",target:"_blank",rel:"noopener noreferrer"}},[t._v("推荐收藏！一文缕清所有大语言模型！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/639026988",target:"_blank",rel:"noopener noreferrer"}},[t._v("最新开源！更擅长推理的LLaMA大模型，支持中文"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/636276475",target:"_blank",rel:"noopener noreferrer"}},[t._v("大模型最全学习笔记： LLM 九层妖塔！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/636274464",target:"_blank",rel:"noopener noreferrer"}},[t._v("田渊栋博士最新研究成果：打开1层Transformer黑盒，注意力机制没那么神秘"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/618346523",target:"_blank",rel:"noopener noreferrer"}},[t._v("学术专用版ChatGPT火了，一键完成论文润色、代码解释、报告生成"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/619529143",target:"_blank",rel:"noopener noreferrer"}},[t._v("五万字综述！Prompt Tuning：深度解读一种新的微调范式"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/619241278",target:"_blank",rel:"noopener noreferrer"}},[t._v("一位哈佛博士分享了他用 GPT-4 搞科研，细到每个工作流程！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/601520822",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习调参（炼丹）指南，一天收获1500星！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/484451643",target:"_blank",rel:"noopener noreferrer"}},[t._v("李宏毅《机器学习》国语课程(2022)来了！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/469782117",target:"_blank",rel:"noopener noreferrer"}},[t._v("北大《深度强化学习中文版》.pdf 开放下载！"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/466406430",target:"_blank",rel:"noopener noreferrer"}},[t._v("吴恩达：告别，大数据"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/465177467",target:"_blank",rel:"noopener noreferrer"}},[t._v("AAAI 2022 | 时间序列相关论文一览（附原文源码）"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/463734679",target:"_blank",rel:"noopener noreferrer"}},[t._v("我删掉了Transformer中的这几层，性能反而变好了"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/461925341",target:"_blank",rel:"noopener noreferrer"}},[t._v("吴恩达：28张图全解深度学习知识"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/461924219",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyTorch优化神经网络的17种方法"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/438910751",target:"_blank",rel:"noopener noreferrer"}},[t._v("聊聊恺明大神MAE的成功之处"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("参考来源：")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://blog.gopenai.com/understanding-transformers-a-step-by-step-math-example-part-1-a7809015150a",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://blog.gopenai.com/understanding-transformers-a-step-by-step-math-example-part-1-a7809015150a"),e("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=n.exports}}]);